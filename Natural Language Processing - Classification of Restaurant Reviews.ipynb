{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of Restaurant Reviws. We would like to classify them as a positive or a negative review. The dataset is in a tab seperated file, where the first column is the review and the 2nd colums is an integeter( 0 or 1) where 1 indicates liked and 0 otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "# We need to specify the delimiter as we are using a .tsv file and we set the quoting to 3 which ignores double-quotes\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)\n",
    "rows, columns = dataset.shape\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported our data, we need to some cleaning of the data so that it can be used in our algorithms. To help in our classification process we shall follow the below preprocessing steps.\n",
    "    * Remove words like 'the','and','so', etc. \n",
    "    * Remove the punctuation and numbers. \n",
    "    * Perform stemming which is a process of removing/cutting different versions of the same words such as 'loved','loves','lover', etc to 'love'. \n",
    "    * Convert all the words to lower case.\n",
    "    * Tokenize the text. This splits all the sentences to seperate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Kiran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "# Import library to help in cleaning text \n",
    "import re\n",
    "# Import library to help in nlp \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at preprocessing just 1 review and later run all the preprocessing steps in a for loop to clean all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "review = dataset['Review'][0]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "# Removing all characters which are not letters\n",
    "review = re.sub('[^a-zA-Z]',' ', review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "# Convert all letters to lower case\n",
    "review = review.lower()\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  ['wow', 'loved', 'this', 'place']\n",
      "After:  ['wow', 'loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "# Splitting the words in the sentence and removing irrelavant words like 'this','the','that' etc\n",
    "review = review.split()\n",
    "print(\"Before: \",review)\n",
    "review = [word for word in review if word not in set(stopwords.words('english'))]\n",
    "print(\"After: \",review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the words\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow love place\n"
     ]
    }
   ],
   "source": [
    "review = ' '.join(review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean all the lines in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crust good', 'tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price', 'get angri want damn pho', 'honeslti tast fresh', 'potato like rubber could tell made ahead time kept warmer', 'fri great', 'great touch']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0, rows):\n",
    "    review = dataset['Review'][i]\n",
    "    review = re.sub('[^a-zA-Z]',' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bag of words model\n",
    "In the Bag of words model we take all the words in our corpus and create a unique set of words with each word in itz own column. The rows will still be all the reviews. Each cell we represent if the word in that column apprears in the review. We will definately have a lot of cells with 0 in them. Hence this matrix of values is called a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the bag of words as a sparse matrix, each column of words can be considered as an independent variable and we can use the dataset's last column as the dependent variable. This is enough data for us to fit a classification model on this dataset. We are going to fit a Navie Bayes Classifier on the dataset and determine the calssfication accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying Navie Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting classifier to the Training Set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test Values: [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n",
      "y_pred Values: [1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test Set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"y_test Values:\", y_test)                            \n",
    "print(\"y_pred Values:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  73.0 %\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn + fp + fn + tp) * 100\n",
    "print(\"Accuracy: \",accuracy,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
